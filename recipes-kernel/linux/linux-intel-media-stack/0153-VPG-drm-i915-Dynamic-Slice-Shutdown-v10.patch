From 86215a0aee7fe014c483b00e5f178fbab8803281 Mon Sep 17 00:00:00 2001
From: Zhipeng Gong <zhipeng.gong@intel.com>
Date: Mon, 5 Sep 2016 20:45:06 -0400
Subject: [PATCH 153/153] [VPG]: drm/i915: Dynamic Slice Shutdown v10

UMD queries policy table for the batch buffer and pass slice
configuration to KMD, KMD chooses the maximum slice configuration
and set it for all the contexts KMD keep the configuration
for at least one second.
---
 drivers/gpu/drm/i915/i915_debugfs.c        |   32 ++++--
 drivers/gpu/drm/i915/i915_dma.c            |    4 +-
 drivers/gpu/drm/i915/i915_drv.h            |   32 ++++++-
 drivers/gpu/drm/i915/i915_gem_context.c    |   15 +++
 drivers/gpu/drm/i915/i915_gem_execbuffer.c |    5 +
 drivers/gpu/drm/i915/i915_params.c         |    5 +
 drivers/gpu/drm/i915/i915_trace.h          |   32 ++++++-
 drivers/gpu/drm/i915/intel_lrc.c           |  153 ++++++++++++++++++++--------
 include/uapi/drm/i915_drm.h                |    2 +
 9 files changed, 221 insertions(+), 59 deletions(-)

diff --git a/drivers/gpu/drm/i915/i915_debugfs.c b/drivers/gpu/drm/i915/i915_debugfs.c
index ef09fdf..97e6790 100644
--- a/drivers/gpu/drm/i915/i915_debugfs.c
+++ b/drivers/gpu/drm/i915/i915_debugfs.c
@@ -5834,10 +5834,8 @@ i915_slice_enabled_get(void *data, u64 *val)
 {
 	struct drm_device *dev = data;
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct intel_device_info *info;
 
-	info = (struct intel_device_info *)&dev_priv->info;
-	*val = (u64) (info->slice_enabled);
+	*val = dev_priv->ruling_sseu;
 	return 0;
 }
 
@@ -5846,17 +5844,29 @@ i915_slice_enabled_set(void *data, u64 val)
 {
 	struct drm_device *dev = data;
 	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct intel_device_info *info;
+	struct intel_device_info *info = (struct intel_device_info *)&dev_priv->info;
+	int ret;
 
-	info = (struct intel_device_info *)&dev_priv->info;
-	if (!info->has_slice_pg)
-		return -EINVAL;
+	ret = mutex_lock_interruptible(&dev->struct_mutex);
+	if (ret)
+		return ret;
 
-	if (val > info->slice_total || val <= 0)
-		return -EINVAL;
+	if (!info->has_slice_pg || val > info->slice_total) {
+		ret = -EINVAL;
+		goto out;
+	}
+
+	if (val == 0) {
+		dev_priv->sseu_override = 0;
+	} else {
+		dev_priv->sseu_override = 1;
+	}
+	dev_priv->ruling_sseu = (u32)val;
+
+out:
+	mutex_unlock(&dev->struct_mutex);
+	return ret;
 
-	info->slice_enabled = (u8)val;
-	return 0;
 }
 
 DEFINE_SIMPLE_ATTRIBUTE(i915_slice_enabled_fops,
diff --git a/drivers/gpu/drm/i915/i915_dma.c b/drivers/gpu/drm/i915/i915_dma.c
index b4b7a17..6003a3d 100644
--- a/drivers/gpu/drm/i915/i915_dma.c
+++ b/drivers/gpu/drm/i915/i915_dma.c
@@ -220,6 +220,9 @@ static int i915_getparam(struct drm_device *dev, void *data,
 	case I915_PRIVATE_PARAM_SCHEDULER_SUPPORT_USER_CTX:
 		value = 1;
 		break;
+	case I915_PRIVATE_PARAM_HAS_DSS_SUPPORT:
+		value = 1;
+		break;
 	default:
 		DRM_DEBUG("Unknown parameter %d\n", param->param);
 		return -EINVAL;
@@ -869,7 +872,6 @@ static void intel_device_info_runtime_init(struct drm_device *dev)
 	else if (INTEL_INFO(dev)->gen >= 9)
 		gen9_sseu_info_init(dev);
 
-	info->slice_enabled = info->slice_total;
 	DRM_DEBUG_DRIVER("slice total: %u\n", info->slice_total);
 	DRM_DEBUG_DRIVER("subslice total: %u\n", info->subslice_total);
 	DRM_DEBUG_DRIVER("subslice per slice: %u\n", info->subslice_per_slice);
diff --git a/drivers/gpu/drm/i915/i915_drv.h b/drivers/gpu/drm/i915/i915_drv.h
index 1b15533..52df753 100644
--- a/drivers/gpu/drm/i915/i915_drv.h
+++ b/drivers/gpu/drm/i915/i915_drv.h
@@ -817,7 +817,6 @@ struct intel_device_info {
 	u8 has_slice_pg:1;
 	u8 has_subslice_pg:1;
 	u8 has_eu_pg:1;
-	u8 slice_enabled;
 };
 
 #undef DEFINE_FLAG
@@ -935,6 +934,8 @@ struct intel_context {
 
 	/* perfmon configuration */
 	struct drm_i915_perfmon_context perfmon;
+	u32 umd_sseu;
+	u32 kmd_sseu;
 };
 
 enum fb_op_origin {
@@ -2047,6 +2048,10 @@ struct drm_i915_private {
 
 	uint32_t request_uniq;
 
+	u8 sseu_override;
+	u32 ruling_sseu;
+	unsigned long ruling_jiffies;
+
 	/*
 	 * NOTE: This is the dri1/ums dungeon, don't add stuff here. Your patch
 	 * will be rejected. Instead look for a better place.
@@ -2384,6 +2389,30 @@ struct drm_i915_gem_request {
 	uint32_t dep_uniq;
 
 	struct i915_mvp_req_record mvp_req;
+
+#define I915_SSEU_CNT_MASK	0xff
+#define I915_SSEU_SLICE_SHIFT	16
+#define I915_SSEU_SS_SHIFT	8
+#define I915_SSEU_EU_SS_SHIFT	0
+#define GET_SSEU_COUNT(setting, s, ss, eu)				      \
+	do {								      \
+		s =  (setting >> I915_SSEU_SLICE_SHIFT) & I915_SSEU_CNT_MASK; \
+		ss = (setting >> I915_SSEU_SS_SHIFT) & I915_SSEU_CNT_MASK; \
+		eu = (setting >> I915_SSEU_EU_SS_SHIFT) & I915_SSEU_CNT_MASK; \
+	} while (0)
+
+#define SET_SSEU_SETTING(setting, s, ss, eu)		\
+	setting = (eu) << I915_SSEU_EU_SS_SHIFT |	\
+		  (ss) << I915_SSEU_SS_SHIFT |		\
+		  (s)  << I915_SSEU_SLICE_SHIFT
+
+	/**
+	 * [0 - 7]: eu per subslice
+	 * [8 -15]: subslice per slice
+	 * [16-23]: slice count
+	 */
+	u32 umd_sseu;
+	u32 kmd_sseu;
 };
 
 int i915_gem_request_alloc(struct intel_engine_cs *ring,
@@ -2810,6 +2839,7 @@ struct i915_params {
 	int edp_vswing;
 	int enable_scheduler;
 	int ring_multiplier;
+	int enable_dss;
 };
 extern struct i915_params i915 __read_mostly;
 
diff --git a/drivers/gpu/drm/i915/i915_gem_context.c b/drivers/gpu/drm/i915/i915_gem_context.c
index 3adfc8e..40b171f 100644
--- a/drivers/gpu/drm/i915/i915_gem_context.c
+++ b/drivers/gpu/drm/i915/i915_gem_context.c
@@ -1020,6 +1020,8 @@ int i915_gem_context_getparam_ioctl(struct drm_device *dev, void *data,
 	case I915_CONTEXT_PRIVATE_PARAM_BOOST:
 		args->value = ctx->flags & CONTEXT_BOOST_FREQ;
 		break;
+	case I915_CONTEXT_PRIVATE_PARAM_SSEU:
+		args->value = ctx->umd_sseu;
 	default:
 		ret = -EINVAL;
 		break;
@@ -1106,6 +1108,19 @@ int i915_gem_context_setparam_ioctl(struct drm_device *dev, void *data,
 		break;
 	}
 
+	case I915_CONTEXT_PRIVATE_PARAM_SSEU:
+	{
+		u32 val = (u32)args->value;
+		struct drm_i915_private *dev_priv  = dev->dev_private;
+		struct intel_device_info *info = (struct intel_device_info *)&dev_priv->info;
+
+		if (val > info->slice_total)
+			ret = -EINVAL;
+		else
+			ctx->umd_sseu = val;
+		break;
+	}
+
 	default:
 		ret = -EINVAL;
 		break;
diff --git a/drivers/gpu/drm/i915/i915_gem_execbuffer.c b/drivers/gpu/drm/i915/i915_gem_execbuffer.c
index ebaecd1..55ecc35 100644
--- a/drivers/gpu/drm/i915/i915_gem_execbuffer.c
+++ b/drivers/gpu/drm/i915/i915_gem_execbuffer.c
@@ -1749,6 +1749,11 @@ i915_gem_do_execbuffer(struct drm_device *dev, void *data,
 	params->args_DR1                = args->DR1;
 	params->args_DR4                = args->DR4;
 	params->batch_obj               = batch_obj;
+	if (ring->id == RCS) {
+		params->request->umd_sseu   = ctx->umd_sseu;
+		params->request->kmd_sseu   = dev_priv->ruling_sseu;
+	}
+
 	i915_mvp_init_req(params->request, args->DR4);
 
 	/* Start with the context's priority level */
diff --git a/drivers/gpu/drm/i915/i915_params.c b/drivers/gpu/drm/i915/i915_params.c
index a0b4825..5767fe7 100644
--- a/drivers/gpu/drm/i915/i915_params.c
+++ b/drivers/gpu/drm/i915/i915_params.c
@@ -57,6 +57,7 @@ struct i915_params i915 __read_mostly = {
 	.guc_log_level = -1,
 	.enable_scheduler = 1,
 	.ring_multiplier = 2,
+	.enable_dss = 0,
 };
 
 module_param_named(modeset, i915.modeset, int, 0400);
@@ -204,3 +205,7 @@ MODULE_PARM_DESC(enable_scheduler, "Enable scheduler (0 = disable, 1 = enable [d
 
 module_param_named(ring_multiplier, i915.ring_multiplier, int, 0400);
 MODULE_PARM_DESC(ring_multiplier, "Configure Ring/GT multiplier for SKL. (2:multiplier is 2 (default), 3:multiplier is 3)");
+
+module_param_named(enable_dss, i915.enable_dss, int, 0600);
+MODULE_PARM_DESC(enable_dss,
+		 "Enable dynamic slice shutdown (0 = disable [default], 1 = enable)");
diff --git a/drivers/gpu/drm/i915/i915_trace.h b/drivers/gpu/drm/i915/i915_trace.h
index a2e98ef..30174a8 100644
--- a/drivers/gpu/drm/i915/i915_trace.h
+++ b/drivers/gpu/drm/i915/i915_trace.h
@@ -1033,6 +1033,8 @@ TRACE_EVENT(i915_mvp_read_req,
 			     __field(u64, gpu_time)
 			     __field(u64, start)
 			     __field(u64, end)
+			     __field(u32, umd_sseu)
+			     __field(u32, kmd_sseu)
 			     ),
 
 	    TP_fast_assign(
@@ -1045,13 +1047,39 @@ TRACE_EVENT(i915_mvp_read_req,
 			   __entry->gpu_time = req->mvp_req.gpu_time;
 			   __entry->start = req->mvp_req.cpu_addr->start_time;
 			   __entry->end = req->mvp_req.cpu_addr->end_time;
+			   __entry->umd_sseu = req->umd_sseu;
+			   __entry->kmd_sseu = req->kmd_sseu;
 			   ),
 
-	    TP_printk("pid=%d tag=%x uniq=%u seqno=%u ring=%d cpu_time=%llx gpu_time=%llx start=%llx end=%llx",
+	    TP_printk("pid=%d tag=%x uniq=%u seqno=%u ring=%d cpu_time=%llx gpu_time=%llx start=%llx end=%llx umd_sseu=%x kmd_sseu=%x",
 		      __entry->pid, __entry->tag, __entry->uniq, __entry->seqno, __entry->ring,
-		      __entry->cpu_time, __entry->gpu_time, __entry->start, __entry->end)
+		      __entry->cpu_time, __entry->gpu_time, __entry->start, __entry->end,
+		      __entry->umd_sseu, __entry->kmd_sseu)
 );
 
+TRACE_EVENT(i915_sseu_change,
+	    TP_PROTO(struct drm_i915_gem_request *req,
+			u32 ruling_sseu, u64 ruling_jiffies),
+	    TP_ARGS(req, ruling_sseu, ruling_jiffies),
+
+	    TP_STRUCT__entry(
+			     __field(struct intel_context *, ctx)
+			     __field(u32, old_sseu)
+			     __field(u32, ruling_sseu)
+			     __field(u64, ruling_jiffies)
+			     __field(u64, new_jiffies)
+			     ),
+
+	    TP_fast_assign(
+			   __entry->ctx = req->ctx;
+			   __entry->old_sseu = req->ctx->kmd_sseu;
+			   __entry->ruling_sseu = ruling_sseu;
+			   __entry->ruling_jiffies = ruling_jiffies;
+			   ),
+
+	    TP_printk("ctx=%p old_sseu=%x ruling_sseu=%x ruling_jiffies=%llx",
+		      __entry->ctx, __entry->old_sseu, __entry->ruling_sseu, __entry->ruling_jiffies)
+);
 
 #endif /* _I915_TRACE_H_ */
 
diff --git a/drivers/gpu/drm/i915/intel_lrc.c b/drivers/gpu/drm/i915/intel_lrc.c
index 4a067a2..9eb6990 100644
--- a/drivers/gpu/drm/i915/intel_lrc.c
+++ b/drivers/gpu/drm/i915/intel_lrc.c
@@ -409,16 +409,113 @@ static void execlists_elsp_write(struct drm_i915_gem_request *rq0,
 	spin_unlock(&dev_priv->uncore.lock);
 }
 
+static u32 make_rpcs(struct drm_device *dev, u32 sseu_setting)
+{
+	u32 rpcs = 0;
+	u8 s, ss, eu;
+
+	/*
+	 * No explicit RPCS request is needed to ensure full
+	 * slice/subslice/EU enablement prior to Gen9.
+	 */
+	if (INTEL_INFO(dev)->gen < 8)
+		return 0;
+
+	GET_SSEU_COUNT(sseu_setting, s, ss, eu);
+
+	if (s == 0 || s > INTEL_INFO(dev)->slice_total)
+		s = INTEL_INFO(dev)->slice_total;
+
+	if (ss == 0 || ss > INTEL_INFO(dev)->subslice_per_slice)
+		ss = INTEL_INFO(dev)->subslice_per_slice;
+
+	if (eu == 0 || eu > INTEL_INFO(dev)->eu_per_subslice)
+		eu = INTEL_INFO(dev)->eu_per_subslice;
+
+	/*
+	 * Starting in Gen9, render power gating can leave
+	 * slice/subslice/EU in a partially enabled state. We
+	 * must make an explicit request through RPCS for full
+	 * enablement.
+	 */
+	if (INTEL_INFO(dev)->has_slice_pg) {
+		rpcs |= GEN8_RPCS_S_CNT_ENABLE;
+		rpcs |= s << GEN8_RPCS_S_CNT_SHIFT;
+		rpcs |= GEN8_RPCS_ENABLE;
+	}
+
+	if (INTEL_INFO(dev)->has_subslice_pg) {
+		rpcs |= GEN8_RPCS_SS_CNT_ENABLE;
+		rpcs |= ss << GEN8_RPCS_SS_CNT_SHIFT;
+		rpcs |= GEN8_RPCS_ENABLE;
+	}
+
+	if (INTEL_INFO(dev)->has_eu_pg) {
+		rpcs |= eu << GEN8_RPCS_EU_MIN_SHIFT;
+		rpcs |= eu << GEN8_RPCS_EU_MAX_SHIFT;
+		rpcs |= GEN8_RPCS_ENABLE;
+	}
+
+	return rpcs;
+}
+
+static u32 make_rpcs_default(struct drm_device *dev)
+{
+	u32 sseu_setting;
+
+	SET_SSEU_SETTING(sseu_setting,
+			 INTEL_INFO(dev)->slice_total,
+			 INTEL_INFO(dev)->subslice_per_slice,
+			 INTEL_INFO(dev)->eu_per_subslice);
+	return make_rpcs(dev, sseu_setting);
+}
+
+static void execlists_get_sseu_state(struct drm_device *dev,
+				struct drm_i915_gem_request *rq)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+
+	u32 sseu_setting;
+
+	if (dev_priv->sseu_override)
+		return;
+
+	/* DSS is disabled, return 0 */
+	if (i915.enable_dss == 0) {
+		dev_priv->ruling_sseu = 0;
+		return;
+	}
+
+	sseu_setting = rq->umd_sseu;
+
+	/*
+	 * The higher UMD requested slice states take presidence over lower UMD
+	 * slice state requests as long as it has been less than one second
+	 * since the higher slice state was last requested
+	 */
+	if ((sseu_setting >= dev_priv->ruling_sseu) ||
+		time_after(jiffies, dev_priv->ruling_jiffies + HZ)) {
+		dev_priv->ruling_sseu = sseu_setting;
+		dev_priv->ruling_jiffies = jiffies;
+	}
+	rq->kmd_sseu = dev_priv->ruling_sseu;
+}
+
 static int execlists_update_context(struct drm_i915_gem_request *rq)
 {
 	struct intel_engine_cs *ring = rq->ring;
 	struct i915_hw_ppgtt *ppgtt = rq->ctx->ppgtt;
 	struct drm_i915_gem_object *ctx_obj = rq->ctx->engine[ring->id].state;
 	struct drm_i915_gem_object *rb_obj = rq->ringbuf->obj;
+	struct drm_i915_private *dev_priv = ring->dev->dev_private;
 	struct page *page;
 	uint32_t *reg_state;
+	u32 sseu_setting;
 
 	BUG_ON(!ctx_obj);
+	if (ring->id == RCS) {
+		execlists_get_sseu_state(ring->dev, rq);
+	}
 
 	page = i915_gem_object_get_dirty_page(ctx_obj, LRC_STATE_PN);
 	reg_state = kmap_atomic(page);
@@ -438,6 +535,17 @@ static int execlists_update_context(struct drm_i915_gem_request *rq)
 		ASSIGN_CTX_PDP(ppgtt, reg_state, 0);
 	}
 
+	if (ring->id == RCS && rq->ctx->kmd_sseu != dev_priv->ruling_sseu) {
+		SET_SSEU_SETTING(sseu_setting,
+				 dev_priv->ruling_sseu,
+				 INTEL_INFO(ring->dev)->subslice_per_slice,
+				 INTEL_INFO(ring->dev)->eu_per_subslice);
+		reg_state[CTX_R_PWR_CLK_STATE + 1] =
+					make_rpcs(ring->dev, sseu_setting);
+		trace_i915_sseu_change(rq, dev_priv->ruling_sseu, dev_priv->ruling_jiffies);
+		rq->ctx->kmd_sseu = dev_priv->ruling_sseu;
+	}
+
 	kunmap_atomic(reg_state);
 
 	return 0;
@@ -2372,49 +2480,6 @@ cleanup_render_ring:
 	return ret;
 }
 
-static u32
-make_rpcs(struct drm_device *dev)
-{
-	u32 rpcs = 0;
-
-	/*
-	 * No explicit RPCS request is needed to ensure full
-	 * slice/subslice/EU enablement prior to Gen9.
-	*/
-	if (INTEL_INFO(dev)->gen < 8)
-		return 0;
-
-	/*
-	 * Starting in Gen9, render power gating can leave
-	 * slice/subslice/EU in a partially enabled state. We
-	 * must make an explicit request through RPCS for full
-	 * enablement.
-	*/
-	if (INTEL_INFO(dev)->has_slice_pg) {
-		rpcs |= GEN8_RPCS_S_CNT_ENABLE;
-		rpcs |= INTEL_INFO(dev)->slice_enabled <<
-			GEN8_RPCS_S_CNT_SHIFT;
-		rpcs |= GEN8_RPCS_ENABLE;
-	}
-
-	if (INTEL_INFO(dev)->has_subslice_pg) {
-		rpcs |= GEN8_RPCS_SS_CNT_ENABLE;
-		rpcs |= INTEL_INFO(dev)->subslice_per_slice <<
-			GEN8_RPCS_SS_CNT_SHIFT;
-		rpcs |= GEN8_RPCS_ENABLE;
-	}
-
-	if (INTEL_INFO(dev)->has_eu_pg) {
-		rpcs |= INTEL_INFO(dev)->eu_per_subslice <<
-			GEN8_RPCS_EU_MIN_SHIFT;
-		rpcs |= INTEL_INFO(dev)->eu_per_subslice <<
-			GEN8_RPCS_EU_MAX_SHIFT;
-		rpcs |= GEN8_RPCS_ENABLE;
-	}
-
-	return rpcs;
-}
-
 static u32 intel_lr_indirect_ctx_offset(struct intel_engine_cs *ring)
 {
 	u32 indirect_ctx_offset;
@@ -2564,7 +2629,7 @@ populate_lr_context(struct intel_context *ctx, struct drm_i915_gem_object *ctx_o
 	if (ring->id == RCS) {
 		reg_state[CTX_LRI_HEADER_2] = MI_LOAD_REGISTER_IMM(1);
 		reg_state[CTX_R_PWR_CLK_STATE] = GEN8_R_PWR_CLK_STATE;
-		reg_state[CTX_R_PWR_CLK_STATE+1] = make_rpcs(dev);
+		reg_state[CTX_R_PWR_CLK_STATE+1] = make_rpcs_default(dev);
 	}
 
 	kunmap_atomic(reg_state);
diff --git a/include/uapi/drm/i915_drm.h b/include/uapi/drm/i915_drm.h
index b455f5b..1e323a6 100644
--- a/include/uapi/drm/i915_drm.h
+++ b/include/uapi/drm/i915_drm.h
@@ -366,6 +366,7 @@ typedef struct drm_i915_irq_wait {
 #define I915_PARAM_HAS_RESOURCE_STREAMER 36
 #define I915_PARAM_HAS_EXEC_SOFTPIN	 37
 
+#define I915_PRIVATE_PARAM_HAS_DSS_SUPPORT	       (-3)
 #define I915_PRIVATE_PARAM_SCHEDULER_SUPPORT_USER_CTX  (-2)
 #define I915_PRIVATE_PARAM_HAS_EXEC_FORCE_NON_COHERENT (-1)
 
@@ -1171,6 +1172,7 @@ struct drm_i915_gem_context_param {
 #define I915_CONTEXT_PARAM_NO_ZEROMAP 0x2
 #define I915_CONTEXT_PARAM_PRIORITY	0x4
 #define I915_CONTEXT_PRIVATE_PARAM_BOOST 0x80000000
+#define I915_CONTEXT_PRIVATE_PARAM_SSEU 0x80000001
 	__u64 value;
 };
 
-- 
1.7.1

